{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sklearn as sks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq_Of_Word_1</th>\n",
       "      <th>Freq_Of_Word_2</th>\n",
       "      <th>Freq_Of_Word_3</th>\n",
       "      <th>Freq_Of_Word_4</th>\n",
       "      <th>Freq_Of_Word_5</th>\n",
       "      <th>Freq_Of_Word_6</th>\n",
       "      <th>Freq_Of_Word_7</th>\n",
       "      <th>Freq_Of_Word_8</th>\n",
       "      <th>Freq_Of_Word_9</th>\n",
       "      <th>Freq_Of_Word_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Freq_Of_Word_45</th>\n",
       "      <th>Freq_Of_Word_46</th>\n",
       "      <th>Freq_Of_Word_47</th>\n",
       "      <th>Freq_Of_Word_48</th>\n",
       "      <th>Freq_Of_Word_49</th>\n",
       "      <th>Freq_Of_Word_50</th>\n",
       "      <th>TotalEmojiCharacters</th>\n",
       "      <th>LengthOFFirstParagraph</th>\n",
       "      <th>StylizedLetters</th>\n",
       "      <th>IsGoodNews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>2.620660</td>\n",
       "      <td>1.253645</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>0.157385</td>\n",
       "      <td>-0.028751</td>\n",
       "      <td>-0.046474</td>\n",
       "      <td>0.222453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>3.837751</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.195476</td>\n",
       "      <td>-0.408024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.187634</td>\n",
       "      <td>-0.392578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.210190</td>\n",
       "      <td>2.682528</td>\n",
       "      <td>1.291868</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>0.221744</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>0.859101</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>2.374782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.340723</td>\n",
       "      <td>1.334201</td>\n",
       "      <td>2.270899</td>\n",
       "      <td>0.602985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.930461</td>\n",
       "      <td>-0.028751</td>\n",
       "      <td>-0.113133</td>\n",
       "      <td>-0.128592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>4.507613</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>3.285692</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>1.829232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.105439</td>\n",
       "      <td>0.154723</td>\n",
       "      <td>0.326031</td>\n",
       "      <td>-0.198801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>7.477691</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.211160</td>\n",
       "      <td>-0.413640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>0.164287</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>1.180616</td>\n",
       "      <td>0.963226</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>0.194633</td>\n",
       "      <td>0.372763</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.359018</td>\n",
       "      <td>-0.002540</td>\n",
       "      <td>0.188792</td>\n",
       "      <td>0.009017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>0.794968</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.084010</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.219003</td>\n",
       "      <td>-0.385557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.180660</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>0.538323</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>0.839509</td>\n",
       "      <td>-0.126789</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.097448</td>\n",
       "      <td>0.848716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Freq_Of_Word_1  Freq_Of_Word_2  Freq_Of_Word_3  Freq_Of_Word_4  \\\n",
       "0       -0.351864        2.620660        1.253645       -0.039223   \n",
       "1       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "2       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "3        1.210190        2.682528        1.291868       -0.039223   \n",
       "4       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "5       -0.351864        4.507613       -0.561952       -0.039223   \n",
       "6       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "7       -0.351864       -0.318036        0.164287       -0.039223   \n",
       "8       -0.351864       -0.318036        0.794968       -0.039223   \n",
       "9       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "\n",
       "   Freq_Of_Word_5  Freq_Of_Word_6  Freq_Of_Word_7  Freq_Of_Word_8  \\\n",
       "0       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "1       -0.465210       -0.353977       -0.304257        3.837751   \n",
       "2       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "3        0.221744       -0.353977       -0.304257        0.859101   \n",
       "4       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "5       -0.465210       -0.353977        3.285692       -0.240708   \n",
       "6        7.477691       -0.353977       -0.304257       -0.240708   \n",
       "7        1.180616        0.963226       -0.304257        0.194633   \n",
       "8       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "9       -0.465210       -0.180660       -0.304257        0.538323   \n",
       "\n",
       "   Freq_Of_Word_9  Freq_Of_Word_10  ...  Freq_Of_Word_45  Freq_Of_Word_46  \\\n",
       "0       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "1       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "2       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "3       -0.318797         2.374782  ...        -0.323019        -0.205212   \n",
       "4       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "5       -0.318797         1.829232  ...        -0.323019        -0.205212   \n",
       "6       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "7        0.372763        -0.352968  ...        -0.323019        -0.205212   \n",
       "8       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "9       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "\n",
       "   Freq_Of_Word_47  Freq_Of_Word_48  Freq_Of_Word_49  Freq_Of_Word_50  \\\n",
       "0        -0.079531        -0.118688         0.079303         0.157385   \n",
       "1        -0.079531        -0.118688        -0.151911        -0.453742   \n",
       "2        -0.079531        -0.118688        -0.151911        -0.453742   \n",
       "3        -0.079531        -0.118688        -0.151911         0.340723   \n",
       "4        -0.079531        -0.118688        -0.151911         0.930461   \n",
       "5        -0.079531        -0.118688        -0.151911         0.105439   \n",
       "6        -0.079531        -0.118688        -0.151911        -0.453742   \n",
       "7        -0.079531        -0.118688        -0.151911        -0.359018   \n",
       "8        -0.079531        -0.118688        -0.151911        -0.084010   \n",
       "9        -0.079531        -0.118688         0.839509        -0.126789   \n",
       "\n",
       "   TotalEmojiCharacters  LengthOFFirstParagraph  StylizedLetters  IsGoodNews  \n",
       "0             -0.028751               -0.046474         0.222453           1  \n",
       "1             -0.107383               -0.195476        -0.408024           0  \n",
       "2             -0.107383               -0.187634        -0.392578           0  \n",
       "3              1.334201                2.270899         0.602985           1  \n",
       "4             -0.028751               -0.113133        -0.128592           0  \n",
       "5              0.154723                0.326031        -0.198801           1  \n",
       "6             -0.107383               -0.211160        -0.413640           0  \n",
       "7             -0.002540                0.188792         0.009017           1  \n",
       "8             -0.107383               -0.219003        -0.385557           0  \n",
       "9             -0.107383               -0.097448         0.848716           0  \n",
       "\n",
       "[10 rows x 54 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 947 entries, 0 to 946\n",
      "Data columns (total 54 columns):\n",
      "Freq_Of_Word_1            947 non-null float64\n",
      "Freq_Of_Word_2            947 non-null float64\n",
      "Freq_Of_Word_3            947 non-null float64\n",
      "Freq_Of_Word_4            947 non-null float64\n",
      "Freq_Of_Word_5            947 non-null float64\n",
      "Freq_Of_Word_6            947 non-null float64\n",
      "Freq_Of_Word_7            947 non-null float64\n",
      "Freq_Of_Word_8            947 non-null float64\n",
      "Freq_Of_Word_9            947 non-null float64\n",
      "Freq_Of_Word_10           947 non-null float64\n",
      "Freq_Of_Word_11           947 non-null float64\n",
      "Freq_Of_Word_12           947 non-null float64\n",
      "Freq_Of_Word_13           947 non-null float64\n",
      "Freq_Of_Word_14           947 non-null float64\n",
      "Freq_Of_Word_15           947 non-null float64\n",
      "Freq_Of_Word_16           947 non-null float64\n",
      "Freq_Of_Word_17           947 non-null float64\n",
      "Freq_Of_Word_18           947 non-null float64\n",
      "Freq_Of_Word_19           947 non-null float64\n",
      "Freq_Of_Word_20           947 non-null float64\n",
      "Freq_Of_Word_21           947 non-null float64\n",
      "Freq_Of_Word_22           947 non-null float64\n",
      "Freq_Of_Word_23           947 non-null float64\n",
      "Freq_Of_Word_24           947 non-null float64\n",
      "Freq_Of_Word_25           947 non-null float64\n",
      "Freq_Of_Word_26           947 non-null float64\n",
      "Freq_Of_Word_27           947 non-null float64\n",
      "Freq_Of_Word_28           947 non-null float64\n",
      "Freq_Of_Word_29           947 non-null float64\n",
      "Freq_Of_Word_30           947 non-null float64\n",
      "Freq_Of_Word_31           947 non-null float64\n",
      "Freq_Of_Word_32           947 non-null float64\n",
      "Freq_Of_Word_33           947 non-null float64\n",
      "Freq_Of_Word_34           947 non-null float64\n",
      "Freq_Of_Word_35           947 non-null float64\n",
      "Freq_Of_Word_36           947 non-null float64\n",
      "Freq_Of_Word_37           947 non-null float64\n",
      "Freq_Of_Word_38           947 non-null float64\n",
      "Freq_Of_Word_39           947 non-null float64\n",
      "Freq_Of_Word_40           947 non-null float64\n",
      "Freq_Of_Word_41           947 non-null float64\n",
      "Freq_Of_Word_42           947 non-null float64\n",
      "Freq_Of_Word_43           947 non-null float64\n",
      "Freq_Of_Word_44           947 non-null float64\n",
      "Freq_Of_Word_45           947 non-null float64\n",
      "Freq_Of_Word_46           947 non-null float64\n",
      "Freq_Of_Word_47           947 non-null float64\n",
      "Freq_Of_Word_48           947 non-null float64\n",
      "Freq_Of_Word_49           947 non-null float64\n",
      "Freq_Of_Word_50           947 non-null float64\n",
      "TotalEmojiCharacters      947 non-null float64\n",
      "LengthOFFirstParagraph    947 non-null float64\n",
      "StylizedLetters           947 non-null float64\n",
      "IsGoodNews                947 non-null int64\n",
      "dtypes: float64(53), int64(1)\n",
      "memory usage: 399.6 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_1</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.023323</td>\n",
       "      <td>1.104679</td>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.351864</td>\n",
       "      <td>13.771711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_2</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.039056</td>\n",
       "      <td>1.085628</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>11.065546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_3</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.020516</td>\n",
       "      <td>1.026794</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>0.326735</td>\n",
       "      <td>9.184940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_4</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.013038</td>\n",
       "      <td>1.345090</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>40.442907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_5</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.018424</td>\n",
       "      <td>0.890268</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>0.107252</td>\n",
       "      <td>8.479498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_6</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.970823</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>11.570173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_7</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.006407</td>\n",
       "      <td>0.868676</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>6.461416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_8</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>1.136686</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>25.215295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_9</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.044759</td>\n",
       "      <td>1.179691</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>18.826505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_10</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>1.129019</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>-0.080193</td>\n",
       "      <td>25.078049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_11</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.027260</td>\n",
       "      <td>1.102226</td>\n",
       "      <td>-0.320591</td>\n",
       "      <td>-0.320591</td>\n",
       "      <td>-0.320591</td>\n",
       "      <td>-0.320591</td>\n",
       "      <td>9.323373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_12</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.018509</td>\n",
       "      <td>0.958783</td>\n",
       "      <td>-0.635215</td>\n",
       "      <td>-0.635215</td>\n",
       "      <td>-0.481649</td>\n",
       "      <td>0.253271</td>\n",
       "      <td>5.814534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_13</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.017778</td>\n",
       "      <td>0.794336</td>\n",
       "      <td>-0.307555</td>\n",
       "      <td>-0.307555</td>\n",
       "      <td>-0.307555</td>\n",
       "      <td>-0.307555</td>\n",
       "      <td>8.067715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_14</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.039708</td>\n",
       "      <td>1.164453</td>\n",
       "      <td>-0.184379</td>\n",
       "      <td>-0.184379</td>\n",
       "      <td>-0.184379</td>\n",
       "      <td>-0.184379</td>\n",
       "      <td>15.514450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_15</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.886515</td>\n",
       "      <td>-0.189867</td>\n",
       "      <td>-0.189867</td>\n",
       "      <td>-0.189867</td>\n",
       "      <td>-0.189867</td>\n",
       "      <td>8.904749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_16</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.016547</td>\n",
       "      <td>0.813463</td>\n",
       "      <td>-0.333610</td>\n",
       "      <td>-0.333610</td>\n",
       "      <td>-0.333610</td>\n",
       "      <td>-0.112822</td>\n",
       "      <td>8.043349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_17</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>1.075503</td>\n",
       "      <td>-0.321557</td>\n",
       "      <td>-0.321557</td>\n",
       "      <td>-0.321557</td>\n",
       "      <td>-0.321557</td>\n",
       "      <td>15.059581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_18</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>1.042382</td>\n",
       "      <td>-0.362461</td>\n",
       "      <td>-0.362461</td>\n",
       "      <td>-0.362461</td>\n",
       "      <td>-0.362461</td>\n",
       "      <td>14.496020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_19</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.009023</td>\n",
       "      <td>1.012172</td>\n",
       "      <td>-0.973161</td>\n",
       "      <td>-0.973161</td>\n",
       "      <td>-0.210148</td>\n",
       "      <td>0.541562</td>\n",
       "      <td>9.624247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_20</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.026035</td>\n",
       "      <td>1.050691</td>\n",
       "      <td>-0.191111</td>\n",
       "      <td>-0.191111</td>\n",
       "      <td>-0.191111</td>\n",
       "      <td>-0.191111</td>\n",
       "      <td>14.063614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_21</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.003728</td>\n",
       "      <td>1.009282</td>\n",
       "      <td>-0.707575</td>\n",
       "      <td>-0.707575</td>\n",
       "      <td>-0.454070</td>\n",
       "      <td>0.385118</td>\n",
       "      <td>9.004282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_22</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.012839</td>\n",
       "      <td>1.035742</td>\n",
       "      <td>-0.132948</td>\n",
       "      <td>-0.132948</td>\n",
       "      <td>-0.132948</td>\n",
       "      <td>-0.132948</td>\n",
       "      <td>13.783699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_23</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.041618</td>\n",
       "      <td>0.894288</td>\n",
       "      <td>-0.291683</td>\n",
       "      <td>-0.291683</td>\n",
       "      <td>-0.291683</td>\n",
       "      <td>-0.291683</td>\n",
       "      <td>11.554370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_24</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.047414</td>\n",
       "      <td>0.477595</td>\n",
       "      <td>-0.197881</td>\n",
       "      <td>-0.197881</td>\n",
       "      <td>-0.197881</td>\n",
       "      <td>-0.197881</td>\n",
       "      <td>6.622568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_25</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.043273</td>\n",
       "      <td>0.883399</td>\n",
       "      <td>-0.331713</td>\n",
       "      <td>-0.331713</td>\n",
       "      <td>-0.331713</td>\n",
       "      <td>-0.331713</td>\n",
       "      <td>9.044864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_26</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>1.076350</td>\n",
       "      <td>-0.298801</td>\n",
       "      <td>-0.298801</td>\n",
       "      <td>-0.298801</td>\n",
       "      <td>-0.298801</td>\n",
       "      <td>15.951335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_27</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.027867</td>\n",
       "      <td>1.065251</td>\n",
       "      <td>-0.213736</td>\n",
       "      <td>-0.213736</td>\n",
       "      <td>-0.213736</td>\n",
       "      <td>-0.213736</td>\n",
       "      <td>14.369834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_28</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.017251</td>\n",
       "      <td>1.105143</td>\n",
       "      <td>-0.228932</td>\n",
       "      <td>-0.228932</td>\n",
       "      <td>-0.228932</td>\n",
       "      <td>-0.228932</td>\n",
       "      <td>16.680271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_29</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.010705</td>\n",
       "      <td>0.862914</td>\n",
       "      <td>-0.164988</td>\n",
       "      <td>-0.164988</td>\n",
       "      <td>-0.164988</td>\n",
       "      <td>-0.164988</td>\n",
       "      <td>16.488726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_30</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.014522</td>\n",
       "      <td>0.924056</td>\n",
       "      <td>-0.233062</td>\n",
       "      <td>-0.233062</td>\n",
       "      <td>-0.233062</td>\n",
       "      <td>-0.233062</td>\n",
       "      <td>9.589729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_31</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.030863</td>\n",
       "      <td>0.690955</td>\n",
       "      <td>-0.151049</td>\n",
       "      <td>-0.151049</td>\n",
       "      <td>-0.151049</td>\n",
       "      <td>-0.151049</td>\n",
       "      <td>9.903118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_32</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.005286</td>\n",
       "      <td>0.917108</td>\n",
       "      <td>-0.150852</td>\n",
       "      <td>-0.150852</td>\n",
       "      <td>-0.150852</td>\n",
       "      <td>-0.150852</td>\n",
       "      <td>13.360294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_33</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.003128</td>\n",
       "      <td>1.088130</td>\n",
       "      <td>-0.183048</td>\n",
       "      <td>-0.183048</td>\n",
       "      <td>-0.183048</td>\n",
       "      <td>-0.183048</td>\n",
       "      <td>16.532296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_34</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.007025</td>\n",
       "      <td>0.915835</td>\n",
       "      <td>-0.152367</td>\n",
       "      <td>-0.152367</td>\n",
       "      <td>-0.152367</td>\n",
       "      <td>-0.152367</td>\n",
       "      <td>13.350169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_35</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.004619</td>\n",
       "      <td>1.002324</td>\n",
       "      <td>-0.236402</td>\n",
       "      <td>-0.236402</td>\n",
       "      <td>-0.236402</td>\n",
       "      <td>-0.236402</td>\n",
       "      <td>10.205191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_36</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.017331</td>\n",
       "      <td>0.860243</td>\n",
       "      <td>-0.244919</td>\n",
       "      <td>-0.244919</td>\n",
       "      <td>-0.244919</td>\n",
       "      <td>-0.244919</td>\n",
       "      <td>10.375976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_37</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.005015</td>\n",
       "      <td>0.961354</td>\n",
       "      <td>-0.329374</td>\n",
       "      <td>-0.329374</td>\n",
       "      <td>-0.329374</td>\n",
       "      <td>-0.329374</td>\n",
       "      <td>9.435541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_38</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.968636</td>\n",
       "      <td>-0.055522</td>\n",
       "      <td>-0.055522</td>\n",
       "      <td>-0.055522</td>\n",
       "      <td>-0.055522</td>\n",
       "      <td>26.320640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_39</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.028426</td>\n",
       "      <td>1.266652</td>\n",
       "      <td>-0.181398</td>\n",
       "      <td>-0.181398</td>\n",
       "      <td>-0.181398</td>\n",
       "      <td>-0.181398</td>\n",
       "      <td>22.843193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_40</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>0.968864</td>\n",
       "      <td>-0.187171</td>\n",
       "      <td>-0.187171</td>\n",
       "      <td>-0.187171</td>\n",
       "      <td>-0.187171</td>\n",
       "      <td>13.531820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_41</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.029366</td>\n",
       "      <td>1.133304</td>\n",
       "      <td>-0.133876</td>\n",
       "      <td>-0.133876</td>\n",
       "      <td>-0.133876</td>\n",
       "      <td>-0.133876</td>\n",
       "      <td>16.852516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_42</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.034475</td>\n",
       "      <td>0.802059</td>\n",
       "      <td>-0.168998</td>\n",
       "      <td>-0.168998</td>\n",
       "      <td>-0.168998</td>\n",
       "      <td>-0.168998</td>\n",
       "      <td>12.663881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_43</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.011441</td>\n",
       "      <td>1.016758</td>\n",
       "      <td>-0.208657</td>\n",
       "      <td>-0.208657</td>\n",
       "      <td>-0.208657</td>\n",
       "      <td>-0.208657</td>\n",
       "      <td>15.211013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_44</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.017819</td>\n",
       "      <td>1.215326</td>\n",
       "      <td>-0.120401</td>\n",
       "      <td>-0.120401</td>\n",
       "      <td>-0.120401</td>\n",
       "      <td>-0.120401</td>\n",
       "      <td>24.828506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_45</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.059328</td>\n",
       "      <td>1.169027</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.018022</td>\n",
       "      <td>18.446001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_46</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.005919</td>\n",
       "      <td>0.959135</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>14.997385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_47</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.031999</td>\n",
       "      <td>0.643179</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>13.943676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_48</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>0.821608</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>15.030734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_49</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.033669</td>\n",
       "      <td>1.146482</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>15.146785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_50</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.230681</td>\n",
       "      <td>0.145162</td>\n",
       "      <td>15.670854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalEmojiCharacters</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.013279</td>\n",
       "      <td>0.958807</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.081172</td>\n",
       "      <td>-0.054961</td>\n",
       "      <td>28.750489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LengthOFFirstParagraph</th>\n",
       "      <td>947.0</td>\n",
       "      <td>-0.021817</td>\n",
       "      <td>0.513887</td>\n",
       "      <td>-0.219003</td>\n",
       "      <td>-0.195476</td>\n",
       "      <td>-0.164107</td>\n",
       "      <td>-0.038632</td>\n",
       "      <td>8.419193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StylizedLetters</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.018881</td>\n",
       "      <td>0.997459</td>\n",
       "      <td>-0.427682</td>\n",
       "      <td>-0.374323</td>\n",
       "      <td>-0.277435</td>\n",
       "      <td>-0.039427</td>\n",
       "      <td>12.437402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsGoodNews</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.388596</td>\n",
       "      <td>0.487689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count      mean       std       min       25%  \\\n",
       "Freq_Of_Word_1          947.0  0.023323  1.104679 -0.351864 -0.351864   \n",
       "Freq_Of_Word_2          947.0  0.039056  1.085628 -0.318036 -0.318036   \n",
       "Freq_Of_Word_3          947.0  0.020516  1.026794 -0.561952 -0.561952   \n",
       "Freq_Of_Word_4          947.0  0.013038  1.345090 -0.039223 -0.039223   \n",
       "Freq_Of_Word_5          947.0 -0.018424  0.890268 -0.465210 -0.465210   \n",
       "Freq_Of_Word_6          947.0  0.002831  0.970823 -0.353977 -0.353977   \n",
       "Freq_Of_Word_7          947.0 -0.006407  0.868676 -0.304257 -0.304257   \n",
       "Freq_Of_Word_8          947.0  0.008428  1.136686 -0.240708 -0.240708   \n",
       "Freq_Of_Word_9          947.0  0.044759  1.179691 -0.318797 -0.318797   \n",
       "Freq_Of_Word_10         947.0  0.005193  1.129019 -0.352968 -0.352968   \n",
       "Freq_Of_Word_11         947.0  0.027260  1.102226 -0.320591 -0.320591   \n",
       "Freq_Of_Word_12         947.0 -0.018509  0.958783 -0.635215 -0.635215   \n",
       "Freq_Of_Word_13         947.0 -0.017778  0.794336 -0.307555 -0.307555   \n",
       "Freq_Of_Word_14         947.0  0.039708  1.164453 -0.184379 -0.184379   \n",
       "Freq_Of_Word_15         947.0 -0.024780  0.886515 -0.189867 -0.189867   \n",
       "Freq_Of_Word_16         947.0 -0.016547  0.813463 -0.333610 -0.333610   \n",
       "Freq_Of_Word_17         947.0  0.015771  1.075503 -0.321557 -0.321557   \n",
       "Freq_Of_Word_18         947.0  0.002899  1.042382 -0.362461 -0.362461   \n",
       "Freq_Of_Word_19         947.0 -0.009023  1.012172 -0.973161 -0.973161   \n",
       "Freq_Of_Word_20         947.0  0.026035  1.050691 -0.191111 -0.191111   \n",
       "Freq_Of_Word_21         947.0 -0.003728  1.009282 -0.707575 -0.707575   \n",
       "Freq_Of_Word_22         947.0  0.012839  1.035742 -0.132948 -0.132948   \n",
       "Freq_Of_Word_23         947.0 -0.041618  0.894288 -0.291683 -0.291683   \n",
       "Freq_Of_Word_24         947.0 -0.047414  0.477595 -0.197881 -0.197881   \n",
       "Freq_Of_Word_25         947.0 -0.043273  0.883399 -0.331713 -0.331713   \n",
       "Freq_Of_Word_26         947.0  0.021235  1.076350 -0.298801 -0.298801   \n",
       "Freq_Of_Word_27         947.0  0.027867  1.065251 -0.213736 -0.213736   \n",
       "Freq_Of_Word_28         947.0  0.017251  1.105143 -0.228932 -0.228932   \n",
       "Freq_Of_Word_29         947.0 -0.010705  0.862914 -0.164988 -0.164988   \n",
       "Freq_Of_Word_30         947.0 -0.014522  0.924056 -0.233062 -0.233062   \n",
       "Freq_Of_Word_31         947.0 -0.030863  0.690955 -0.151049 -0.151049   \n",
       "Freq_Of_Word_32         947.0 -0.005286  0.917108 -0.150852 -0.150852   \n",
       "Freq_Of_Word_33         947.0 -0.003128  1.088130 -0.183048 -0.183048   \n",
       "Freq_Of_Word_34         947.0 -0.007025  0.915835 -0.152367 -0.152367   \n",
       "Freq_Of_Word_35         947.0 -0.004619  1.002324 -0.236402 -0.236402   \n",
       "Freq_Of_Word_36         947.0 -0.017331  0.860243 -0.244919 -0.244919   \n",
       "Freq_Of_Word_37         947.0 -0.005015  0.961354 -0.329374 -0.329374   \n",
       "Freq_Of_Word_38         947.0  0.006574  0.968636 -0.055522 -0.055522   \n",
       "Freq_Of_Word_39         947.0  0.028426  1.266652 -0.181398 -0.181398   \n",
       "Freq_Of_Word_40         947.0 -0.000311  0.968864 -0.187171 -0.187171   \n",
       "Freq_Of_Word_41         947.0  0.029366  1.133304 -0.133876 -0.133876   \n",
       "Freq_Of_Word_42         947.0 -0.034475  0.802059 -0.168998 -0.168998   \n",
       "Freq_Of_Word_43         947.0 -0.011441  1.016758 -0.208657 -0.208657   \n",
       "Freq_Of_Word_44         947.0  0.017819  1.215326 -0.120401 -0.120401   \n",
       "Freq_Of_Word_45         947.0  0.059328  1.169027 -0.323019 -0.323019   \n",
       "Freq_Of_Word_46         947.0 -0.005919  0.959135 -0.205212 -0.205212   \n",
       "Freq_Of_Word_47         947.0 -0.031999  0.643179 -0.079531 -0.079531   \n",
       "Freq_Of_Word_48         947.0 -0.001397  0.821608 -0.118688 -0.118688   \n",
       "Freq_Of_Word_49         947.0  0.033669  1.146482 -0.151911 -0.151911   \n",
       "Freq_Of_Word_50         947.0  0.013292  0.860000 -0.453742 -0.453742   \n",
       "TotalEmojiCharacters    947.0 -0.013279  0.958807 -0.107383 -0.107383   \n",
       "LengthOFFirstParagraph  947.0 -0.021817  0.513887 -0.219003 -0.195476   \n",
       "StylizedLetters         947.0  0.018881  0.997459 -0.427682 -0.374323   \n",
       "IsGoodNews              947.0  0.388596  0.487689  0.000000  0.000000   \n",
       "\n",
       "                             50%       75%        max  \n",
       "Freq_Of_Word_1         -0.351864 -0.351864  13.771711  \n",
       "Freq_Of_Word_2         -0.318036 -0.318036  11.065546  \n",
       "Freq_Of_Word_3         -0.561952  0.326735   9.184940  \n",
       "Freq_Of_Word_4         -0.039223 -0.039223  40.442907  \n",
       "Freq_Of_Word_5         -0.465210  0.107252   8.479498  \n",
       "Freq_Of_Word_6         -0.353977 -0.353977  11.570173  \n",
       "Freq_Of_Word_7         -0.304257 -0.304257   6.461416  \n",
       "Freq_Of_Word_8         -0.240708 -0.240708  25.215295  \n",
       "Freq_Of_Word_9         -0.318797 -0.318797  18.826505  \n",
       "Freq_Of_Word_10        -0.352968 -0.080193  25.078049  \n",
       "Freq_Of_Word_11        -0.320591 -0.320591   9.323373  \n",
       "Freq_Of_Word_12        -0.481649  0.253271   5.814534  \n",
       "Freq_Of_Word_13        -0.307555 -0.307555   8.067715  \n",
       "Freq_Of_Word_14        -0.184379 -0.184379  15.514450  \n",
       "Freq_Of_Word_15        -0.189867 -0.189867   8.904749  \n",
       "Freq_Of_Word_16        -0.333610 -0.112822   8.043349  \n",
       "Freq_Of_Word_17        -0.321557 -0.321557  15.059581  \n",
       "Freq_Of_Word_18        -0.362461 -0.362461  14.496020  \n",
       "Freq_Of_Word_19        -0.210148  0.541562   9.624247  \n",
       "Freq_Of_Word_20        -0.191111 -0.191111  14.063614  \n",
       "Freq_Of_Word_21        -0.454070  0.385118   9.004282  \n",
       "Freq_Of_Word_22        -0.132948 -0.132948  13.783699  \n",
       "Freq_Of_Word_23        -0.291683 -0.291683  11.554370  \n",
       "Freq_Of_Word_24        -0.197881 -0.197881   6.622568  \n",
       "Freq_Of_Word_25        -0.331713 -0.331713   9.044864  \n",
       "Freq_Of_Word_26        -0.298801 -0.298801  15.951335  \n",
       "Freq_Of_Word_27        -0.213736 -0.213736  14.369834  \n",
       "Freq_Of_Word_28        -0.228932 -0.228932  16.680271  \n",
       "Freq_Of_Word_29        -0.164988 -0.164988  16.488726  \n",
       "Freq_Of_Word_30        -0.233062 -0.233062   9.589729  \n",
       "Freq_Of_Word_31        -0.151049 -0.151049   9.903118  \n",
       "Freq_Of_Word_32        -0.150852 -0.150852  13.360294  \n",
       "Freq_Of_Word_33        -0.183048 -0.183048  16.532296  \n",
       "Freq_Of_Word_34        -0.152367 -0.152367  13.350169  \n",
       "Freq_Of_Word_35        -0.236402 -0.236402  10.205191  \n",
       "Freq_Of_Word_36        -0.244919 -0.244919  10.375976  \n",
       "Freq_Of_Word_37        -0.329374 -0.329374   9.435541  \n",
       "Freq_Of_Word_38        -0.055522 -0.055522  26.320640  \n",
       "Freq_Of_Word_39        -0.181398 -0.181398  22.843193  \n",
       "Freq_Of_Word_40        -0.187171 -0.187171  13.531820  \n",
       "Freq_Of_Word_41        -0.133876 -0.133876  16.852516  \n",
       "Freq_Of_Word_42        -0.168998 -0.168998  12.663881  \n",
       "Freq_Of_Word_43        -0.208657 -0.208657  15.211013  \n",
       "Freq_Of_Word_44        -0.120401 -0.120401  24.828506  \n",
       "Freq_Of_Word_45        -0.323019 -0.018022  18.446001  \n",
       "Freq_Of_Word_46        -0.205212 -0.205212  14.997385  \n",
       "Freq_Of_Word_47        -0.079531 -0.079531  13.943676  \n",
       "Freq_Of_Word_48        -0.118688 -0.118688  15.030734  \n",
       "Freq_Of_Word_49        -0.151911 -0.151911  15.146785  \n",
       "Freq_Of_Word_50        -0.230681  0.145162  15.670854  \n",
       "TotalEmojiCharacters   -0.081172 -0.054961  28.750489  \n",
       "LengthOFFirstParagraph -0.164107 -0.038632   8.419193  \n",
       "StylizedLetters        -0.277435 -0.039427  12.437402  \n",
       "IsGoodNews              0.000000  1.000000   1.000000  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq_Of_Word_1</th>\n",
       "      <th>Freq_Of_Word_2</th>\n",
       "      <th>Freq_Of_Word_3</th>\n",
       "      <th>Freq_Of_Word_4</th>\n",
       "      <th>Freq_Of_Word_5</th>\n",
       "      <th>Freq_Of_Word_6</th>\n",
       "      <th>Freq_Of_Word_7</th>\n",
       "      <th>Freq_Of_Word_8</th>\n",
       "      <th>Freq_Of_Word_9</th>\n",
       "      <th>Freq_Of_Word_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Freq_Of_Word_44</th>\n",
       "      <th>Freq_Of_Word_45</th>\n",
       "      <th>Freq_Of_Word_46</th>\n",
       "      <th>Freq_Of_Word_47</th>\n",
       "      <th>Freq_Of_Word_48</th>\n",
       "      <th>Freq_Of_Word_49</th>\n",
       "      <th>Freq_Of_Word_50</th>\n",
       "      <th>TotalEmojiCharacters</th>\n",
       "      <th>LengthOFFirstParagraph</th>\n",
       "      <th>StylizedLetters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>527.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.034587</td>\n",
       "      <td>-0.015567</td>\n",
       "      <td>-0.011815</td>\n",
       "      <td>-0.034855</td>\n",
       "      <td>0.048186</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>-0.039067</td>\n",
       "      <td>-0.042885</td>\n",
       "      <td>-0.023401</td>\n",
       "      <td>-0.022341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>-0.046060</td>\n",
       "      <td>0.023906</td>\n",
       "      <td>0.068957</td>\n",
       "      <td>0.025588</td>\n",
       "      <td>-0.014241</td>\n",
       "      <td>-0.051894</td>\n",
       "      <td>0.029887</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>-0.015275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.863755</td>\n",
       "      <td>0.991826</td>\n",
       "      <td>1.008872</td>\n",
       "      <td>0.060872</td>\n",
       "      <td>1.224778</td>\n",
       "      <td>1.193867</td>\n",
       "      <td>1.003432</td>\n",
       "      <td>0.636875</td>\n",
       "      <td>0.847441</td>\n",
       "      <td>0.766367</td>\n",
       "      <td>...</td>\n",
       "      <td>1.042504</td>\n",
       "      <td>0.927857</td>\n",
       "      <td>1.212529</td>\n",
       "      <td>1.285230</td>\n",
       "      <td>1.515122</td>\n",
       "      <td>0.874084</td>\n",
       "      <td>0.771676</td>\n",
       "      <td>1.241018</td>\n",
       "      <td>0.632055</td>\n",
       "      <td>0.697853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120401</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.219003</td>\n",
       "      <td>-0.427682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120401</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.191555</td>\n",
       "      <td>-0.368707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120401</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.258182</td>\n",
       "      <td>-0.081172</td>\n",
       "      <td>-0.156265</td>\n",
       "      <td>-0.274627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.335593</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>0.278956</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>0.143030</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.052216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120401</td>\n",
       "      <td>-0.247943</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.070299</td>\n",
       "      <td>-0.054961</td>\n",
       "      <td>-0.038632</td>\n",
       "      <td>0.004103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.662491</td>\n",
       "      <td>12.550362</td>\n",
       "      <td>7.082669</td>\n",
       "      <td>1.059753</td>\n",
       "      <td>13.846323</td>\n",
       "      <td>20.028000</td>\n",
       "      <td>12.122489</td>\n",
       "      <td>5.785330</td>\n",
       "      <td>5.614063</td>\n",
       "      <td>5.242416</td>\n",
       "      <td>...</td>\n",
       "      <td>20.662038</td>\n",
       "      <td>13.078061</td>\n",
       "      <td>18.783821</td>\n",
       "      <td>16.021188</td>\n",
       "      <td>31.707828</td>\n",
       "      <td>13.763001</td>\n",
       "      <td>12.596883</td>\n",
       "      <td>26.627430</td>\n",
       "      <td>7.783974</td>\n",
       "      <td>4.163982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Freq_Of_Word_1  Freq_Of_Word_2  Freq_Of_Word_3  Freq_Of_Word_4  \\\n",
       "count      527.000000      527.000000      527.000000      527.000000   \n",
       "mean        -0.034587       -0.015567       -0.011815       -0.034855   \n",
       "std          0.863755        0.991826        1.008872        0.060872   \n",
       "min         -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "25%         -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "50%         -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "75%         -0.335593       -0.318036        0.278956       -0.039223   \n",
       "max          8.662491       12.550362        7.082669        1.059753   \n",
       "\n",
       "       Freq_Of_Word_5  Freq_Of_Word_6  Freq_Of_Word_7  Freq_Of_Word_8  \\\n",
       "count      527.000000      527.000000      527.000000      527.000000   \n",
       "mean         0.048186        0.004758       -0.039067       -0.042885   \n",
       "std          1.224778        1.193867        1.003432        0.636875   \n",
       "min         -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "25%         -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "50%         -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "75%          0.143030       -0.353977       -0.304257       -0.240708   \n",
       "max         13.846323       20.028000       12.122489        5.785330   \n",
       "\n",
       "       Freq_Of_Word_9  Freq_Of_Word_10  ...  Freq_Of_Word_44  Freq_Of_Word_45  \\\n",
       "count      527.000000       527.000000  ...       527.000000       527.000000   \n",
       "mean        -0.023401        -0.022341  ...         0.004794        -0.046060   \n",
       "std          0.847441         0.766367  ...         1.042504         0.927857   \n",
       "min         -0.318797        -0.352968  ...        -0.120401        -0.323019   \n",
       "25%         -0.318797        -0.352968  ...        -0.120401        -0.323019   \n",
       "50%         -0.318797        -0.352968  ...        -0.120401        -0.323019   \n",
       "75%         -0.318797        -0.052216  ...        -0.120401        -0.247943   \n",
       "max          5.614063         5.242416  ...        20.662038        13.078061   \n",
       "\n",
       "       Freq_Of_Word_46  Freq_Of_Word_47  Freq_Of_Word_48  Freq_Of_Word_49  \\\n",
       "count       527.000000       527.000000       527.000000       527.000000   \n",
       "mean          0.023906         0.068957         0.025588        -0.014241   \n",
       "std           1.212529         1.285230         1.515122         0.874084   \n",
       "min          -0.205212        -0.079531        -0.118688        -0.151911   \n",
       "25%          -0.205212        -0.079531        -0.118688        -0.151911   \n",
       "50%          -0.205212        -0.079531        -0.118688        -0.151911   \n",
       "75%          -0.205212        -0.079531        -0.118688        -0.151911   \n",
       "max          18.783821        16.021188        31.707828        13.763001   \n",
       "\n",
       "       Freq_Of_Word_50  TotalEmojiCharacters  LengthOFFirstParagraph  \\\n",
       "count       527.000000            527.000000              527.000000   \n",
       "mean         -0.051894              0.029887                0.010378   \n",
       "std           0.771676              1.241018                0.632055   \n",
       "min          -0.453742             -0.107383               -0.219003   \n",
       "25%          -0.453742             -0.107383               -0.191555   \n",
       "50%          -0.258182             -0.081172               -0.156265   \n",
       "75%           0.070299             -0.054961               -0.038632   \n",
       "max          12.596883             26.627430                7.783974   \n",
       "\n",
       "       StylizedLetters  \n",
       "count       527.000000  \n",
       "mean         -0.015275  \n",
       "std           0.697853  \n",
       "min          -0.427682  \n",
       "25%          -0.368707  \n",
       "50%          -0.274627  \n",
       "75%           0.004103  \n",
       "max           4.163982  \n",
       "\n",
       "[8 rows x 53 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 54)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527, 53)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Freq_Of_Word_1            0\n",
       "Freq_Of_Word_2            0\n",
       "Freq_Of_Word_3            0\n",
       "Freq_Of_Word_4            0\n",
       "Freq_Of_Word_5            0\n",
       "Freq_Of_Word_6            0\n",
       "Freq_Of_Word_7            0\n",
       "Freq_Of_Word_8            0\n",
       "Freq_Of_Word_9            0\n",
       "Freq_Of_Word_10           0\n",
       "Freq_Of_Word_11           0\n",
       "Freq_Of_Word_12           0\n",
       "Freq_Of_Word_13           0\n",
       "Freq_Of_Word_14           0\n",
       "Freq_Of_Word_15           0\n",
       "Freq_Of_Word_16           0\n",
       "Freq_Of_Word_17           0\n",
       "Freq_Of_Word_18           0\n",
       "Freq_Of_Word_19           0\n",
       "Freq_Of_Word_20           0\n",
       "Freq_Of_Word_21           0\n",
       "Freq_Of_Word_22           0\n",
       "Freq_Of_Word_23           0\n",
       "Freq_Of_Word_24           0\n",
       "Freq_Of_Word_25           0\n",
       "Freq_Of_Word_26           0\n",
       "Freq_Of_Word_27           0\n",
       "Freq_Of_Word_28           0\n",
       "Freq_Of_Word_29           0\n",
       "Freq_Of_Word_30           0\n",
       "Freq_Of_Word_31           0\n",
       "Freq_Of_Word_32           0\n",
       "Freq_Of_Word_33           0\n",
       "Freq_Of_Word_34           0\n",
       "Freq_Of_Word_35           0\n",
       "Freq_Of_Word_36           0\n",
       "Freq_Of_Word_37           0\n",
       "Freq_Of_Word_38           0\n",
       "Freq_Of_Word_39           0\n",
       "Freq_Of_Word_40           0\n",
       "Freq_Of_Word_41           0\n",
       "Freq_Of_Word_42           0\n",
       "Freq_Of_Word_43           0\n",
       "Freq_Of_Word_44           0\n",
       "Freq_Of_Word_45           0\n",
       "Freq_Of_Word_46           0\n",
       "Freq_Of_Word_47           0\n",
       "Freq_Of_Word_48           0\n",
       "Freq_Of_Word_49           0\n",
       "Freq_Of_Word_50           0\n",
       "TotalEmojiCharacters      0\n",
       "LengthOFFirstParagraph    0\n",
       "StylizedLetters           0\n",
       "IsGoodNews                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Freq_Of_Word_1            0\n",
       "Freq_Of_Word_2            0\n",
       "Freq_Of_Word_3            0\n",
       "Freq_Of_Word_4            0\n",
       "Freq_Of_Word_5            0\n",
       "Freq_Of_Word_6            0\n",
       "Freq_Of_Word_7            0\n",
       "Freq_Of_Word_8            0\n",
       "Freq_Of_Word_9            0\n",
       "Freq_Of_Word_10           0\n",
       "Freq_Of_Word_11           0\n",
       "Freq_Of_Word_12           0\n",
       "Freq_Of_Word_13           0\n",
       "Freq_Of_Word_14           0\n",
       "Freq_Of_Word_15           0\n",
       "Freq_Of_Word_16           0\n",
       "Freq_Of_Word_17           0\n",
       "Freq_Of_Word_18           0\n",
       "Freq_Of_Word_19           0\n",
       "Freq_Of_Word_20           0\n",
       "Freq_Of_Word_21           0\n",
       "Freq_Of_Word_22           0\n",
       "Freq_Of_Word_23           0\n",
       "Freq_Of_Word_24           0\n",
       "Freq_Of_Word_25           0\n",
       "Freq_Of_Word_26           0\n",
       "Freq_Of_Word_27           0\n",
       "Freq_Of_Word_28           0\n",
       "Freq_Of_Word_29           0\n",
       "Freq_Of_Word_30           0\n",
       "Freq_Of_Word_31           0\n",
       "Freq_Of_Word_32           0\n",
       "Freq_Of_Word_33           0\n",
       "Freq_Of_Word_34           0\n",
       "Freq_Of_Word_35           0\n",
       "Freq_Of_Word_36           0\n",
       "Freq_Of_Word_37           0\n",
       "Freq_Of_Word_38           0\n",
       "Freq_Of_Word_39           0\n",
       "Freq_Of_Word_40           0\n",
       "Freq_Of_Word_41           0\n",
       "Freq_Of_Word_42           0\n",
       "Freq_Of_Word_43           0\n",
       "Freq_Of_Word_44           0\n",
       "Freq_Of_Word_45           0\n",
       "Freq_Of_Word_46           0\n",
       "Freq_Of_Word_47           0\n",
       "Freq_Of_Word_48           0\n",
       "Freq_Of_Word_49           0\n",
       "Freq_Of_Word_50           0\n",
       "TotalEmojiCharacters      0\n",
       "LengthOFFirstParagraph    0\n",
       "StylizedLetters           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 54)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq_Of_Word_1            0.000000\n",
      "Freq_Of_Word_2            0.000000\n",
      "Freq_Of_Word_3            0.888687\n",
      "Freq_Of_Word_4            0.000000\n",
      "Freq_Of_Word_5            0.572461\n",
      "Freq_Of_Word_6            0.000000\n",
      "Freq_Of_Word_7            0.000000\n",
      "Freq_Of_Word_8            0.000000\n",
      "Freq_Of_Word_9            0.000000\n",
      "Freq_Of_Word_10           0.272775\n",
      "Freq_Of_Word_11           0.000000\n",
      "Freq_Of_Word_12           0.888486\n",
      "Freq_Of_Word_13           0.000000\n",
      "Freq_Of_Word_14           0.000000\n",
      "Freq_Of_Word_15           0.000000\n",
      "Freq_Of_Word_16           0.220788\n",
      "Freq_Of_Word_17           0.000000\n",
      "Freq_Of_Word_18           0.000000\n",
      "Freq_Of_Word_19           1.514723\n",
      "Freq_Of_Word_20           0.000000\n",
      "Freq_Of_Word_21           1.092693\n",
      "Freq_Of_Word_22           0.000000\n",
      "Freq_Of_Word_23           0.000000\n",
      "Freq_Of_Word_24           0.000000\n",
      "Freq_Of_Word_25           0.000000\n",
      "Freq_Of_Word_26           0.000000\n",
      "Freq_Of_Word_27           0.000000\n",
      "Freq_Of_Word_28           0.000000\n",
      "Freq_Of_Word_29           0.000000\n",
      "Freq_Of_Word_30           0.000000\n",
      "Freq_Of_Word_31           0.000000\n",
      "Freq_Of_Word_32           0.000000\n",
      "Freq_Of_Word_33           0.000000\n",
      "Freq_Of_Word_34           0.000000\n",
      "Freq_Of_Word_35           0.000000\n",
      "Freq_Of_Word_36           0.000000\n",
      "Freq_Of_Word_37           0.000000\n",
      "Freq_Of_Word_38           0.000000\n",
      "Freq_Of_Word_39           0.000000\n",
      "Freq_Of_Word_40           0.000000\n",
      "Freq_Of_Word_41           0.000000\n",
      "Freq_Of_Word_42           0.000000\n",
      "Freq_Of_Word_43           0.000000\n",
      "Freq_Of_Word_44           0.000000\n",
      "Freq_Of_Word_45           0.304997\n",
      "Freq_Of_Word_46           0.000000\n",
      "Freq_Of_Word_47           0.000000\n",
      "Freq_Of_Word_48           0.000000\n",
      "Freq_Of_Word_49           0.000000\n",
      "Freq_Of_Word_50           0.598905\n",
      "TotalEmojiCharacters      0.052421\n",
      "LengthOFFirstParagraph    0.156844\n",
      "StylizedLetters           0.334897\n",
      "IsGoodNews                1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Freq_Of_Word_1  Freq_Of_Word_2  Freq_Of_Word_3  Freq_Of_Word_4  \\\n",
      "0             False           False           False           False   \n",
      "1             False           False           False           False   \n",
      "2             False           False           False           False   \n",
      "3             False           False           False           False   \n",
      "4             False           False           False           False   \n",
      "5             False           False           False           False   \n",
      "6             False           False           False           False   \n",
      "7             False           False           False           False   \n",
      "8             False           False           False           False   \n",
      "9             False           False           False           False   \n",
      "10            False           False           False           False   \n",
      "11            False           False           False           False   \n",
      "12            False           False           False           False   \n",
      "13            False           False           False           False   \n",
      "14            False           False           False           False   \n",
      "15            False           False           False           False   \n",
      "16            False           False           False           False   \n",
      "17            False           False           False           False   \n",
      "18            False           False           False           False   \n",
      "19            False           False           False           False   \n",
      "20            False           False           False           False   \n",
      "21            False           False           False           False   \n",
      "22            False           False           False           False   \n",
      "23            False           False           False           False   \n",
      "24            False           False           False           False   \n",
      "25            False           False           False           False   \n",
      "26            False           False           False           False   \n",
      "27            False           False           False           False   \n",
      "28            False           False           False           False   \n",
      "29            False           False           False           False   \n",
      "..              ...             ...             ...             ...   \n",
      "917           False           False           False           False   \n",
      "918           False           False           False           False   \n",
      "919           False           False           False           False   \n",
      "920           False           False           False           False   \n",
      "921           False           False           False           False   \n",
      "922           False           False           False           False   \n",
      "923           False           False           False           False   \n",
      "924           False           False           False           False   \n",
      "925           False           False           False           False   \n",
      "926           False           False           False           False   \n",
      "927           False           False           False           False   \n",
      "928           False           False           False           False   \n",
      "929           False           False           False           False   \n",
      "930           False           False           False           False   \n",
      "931           False           False           False           False   \n",
      "932           False           False           False           False   \n",
      "933           False           False           False           False   \n",
      "934           False           False           False           False   \n",
      "935           False           False           False           False   \n",
      "936           False           False           False           False   \n",
      "937           False           False           False           False   \n",
      "938           False           False           False           False   \n",
      "939           False           False           False           False   \n",
      "940           False           False           False           False   \n",
      "941           False           False           False           False   \n",
      "942           False           False           False           False   \n",
      "943           False           False           False           False   \n",
      "944           False           False           False           False   \n",
      "945           False           False           False           False   \n",
      "946           False           False           False           False   \n",
      "\n",
      "     Freq_Of_Word_5  Freq_Of_Word_6  Freq_Of_Word_7  Freq_Of_Word_8  \\\n",
      "0             False           False           False           False   \n",
      "1             False           False           False           False   \n",
      "2             False           False           False           False   \n",
      "3             False           False           False           False   \n",
      "4             False           False           False           False   \n",
      "5             False           False           False           False   \n",
      "6             False           False           False           False   \n",
      "7             False           False           False           False   \n",
      "8             False           False           False           False   \n",
      "9             False           False           False           False   \n",
      "10            False           False           False           False   \n",
      "11            False           False           False           False   \n",
      "12            False           False           False           False   \n",
      "13            False           False           False           False   \n",
      "14            False           False           False           False   \n",
      "15            False           False           False           False   \n",
      "16            False           False           False           False   \n",
      "17            False           False           False           False   \n",
      "18            False           False           False           False   \n",
      "19            False           False           False           False   \n",
      "20            False           False           False           False   \n",
      "21            False           False           False           False   \n",
      "22            False           False           False           False   \n",
      "23            False           False           False           False   \n",
      "24            False           False           False           False   \n",
      "25            False           False           False           False   \n",
      "26            False           False           False           False   \n",
      "27            False           False           False           False   \n",
      "28            False           False           False           False   \n",
      "29            False           False           False           False   \n",
      "..              ...             ...             ...             ...   \n",
      "917           False           False           False           False   \n",
      "918           False           False           False           False   \n",
      "919           False           False           False           False   \n",
      "920           False           False           False           False   \n",
      "921           False           False           False           False   \n",
      "922           False           False           False           False   \n",
      "923           False           False           False           False   \n",
      "924           False           False           False           False   \n",
      "925           False           False           False           False   \n",
      "926           False           False           False           False   \n",
      "927           False           False           False           False   \n",
      "928           False           False           False           False   \n",
      "929           False           False           False           False   \n",
      "930           False           False           False           False   \n",
      "931           False           False           False           False   \n",
      "932           False           False           False           False   \n",
      "933           False           False           False           False   \n",
      "934           False           False           False           False   \n",
      "935           False           False           False           False   \n",
      "936           False           False           False           False   \n",
      "937           False           False           False           False   \n",
      "938           False           False           False           False   \n",
      "939           False           False           False           False   \n",
      "940           False           False           False           False   \n",
      "941           False           False           False           False   \n",
      "942           False           False           False           False   \n",
      "943           False           False           False           False   \n",
      "944           False           False           False           False   \n",
      "945           False           False           False           False   \n",
      "946           False           False           False           False   \n",
      "\n",
      "     Freq_Of_Word_9  Freq_Of_Word_10  ...  Freq_Of_Word_45  Freq_Of_Word_46  \\\n",
      "0             False            False  ...            False            False   \n",
      "1             False            False  ...            False            False   \n",
      "2             False            False  ...            False            False   \n",
      "3             False            False  ...            False            False   \n",
      "4             False            False  ...            False            False   \n",
      "5             False            False  ...            False            False   \n",
      "6             False            False  ...            False            False   \n",
      "7             False            False  ...            False            False   \n",
      "8             False            False  ...            False            False   \n",
      "9             False            False  ...            False            False   \n",
      "10            False            False  ...            False            False   \n",
      "11            False            False  ...            False            False   \n",
      "12            False            False  ...            False            False   \n",
      "13            False            False  ...            False            False   \n",
      "14            False            False  ...            False            False   \n",
      "15            False            False  ...            False            False   \n",
      "16            False            False  ...            False            False   \n",
      "17            False            False  ...            False            False   \n",
      "18            False            False  ...            False            False   \n",
      "19            False            False  ...            False            False   \n",
      "20            False            False  ...            False            False   \n",
      "21            False            False  ...            False            False   \n",
      "22            False            False  ...            False            False   \n",
      "23            False            False  ...            False            False   \n",
      "24            False            False  ...            False            False   \n",
      "25            False            False  ...            False            False   \n",
      "26            False            False  ...            False            False   \n",
      "27            False            False  ...            False            False   \n",
      "28            False            False  ...            False            False   \n",
      "29            False            False  ...            False            False   \n",
      "..              ...              ...  ...              ...              ...   \n",
      "917           False            False  ...            False            False   \n",
      "918           False            False  ...            False            False   \n",
      "919           False            False  ...            False            False   \n",
      "920           False            False  ...            False            False   \n",
      "921           False            False  ...            False            False   \n",
      "922           False            False  ...            False            False   \n",
      "923           False            False  ...            False            False   \n",
      "924           False            False  ...            False            False   \n",
      "925           False            False  ...            False            False   \n",
      "926           False            False  ...            False            False   \n",
      "927           False            False  ...            False            False   \n",
      "928           False            False  ...            False            False   \n",
      "929           False            False  ...            False            False   \n",
      "930           False            False  ...            False            False   \n",
      "931           False            False  ...            False            False   \n",
      "932           False            False  ...            False            False   \n",
      "933           False            False  ...            False            False   \n",
      "934           False            False  ...            False            False   \n",
      "935           False            False  ...            False            False   \n",
      "936           False            False  ...            False            False   \n",
      "937           False            False  ...            False            False   \n",
      "938           False            False  ...            False            False   \n",
      "939           False            False  ...            False            False   \n",
      "940           False            False  ...            False            False   \n",
      "941           False            False  ...            False            False   \n",
      "942           False            False  ...            False            False   \n",
      "943           False            False  ...            False            False   \n",
      "944           False            False  ...            False            False   \n",
      "945           False            False  ...            False            False   \n",
      "946           False            False  ...            False            False   \n",
      "\n",
      "     Freq_Of_Word_47  Freq_Of_Word_48  Freq_Of_Word_49  Freq_Of_Word_50  \\\n",
      "0              False            False            False            False   \n",
      "1              False            False            False            False   \n",
      "2              False            False            False            False   \n",
      "3              False            False            False            False   \n",
      "4              False            False            False            False   \n",
      "5              False            False            False            False   \n",
      "6              False            False            False            False   \n",
      "7              False            False            False            False   \n",
      "8              False            False            False            False   \n",
      "9              False            False            False            False   \n",
      "10             False            False            False            False   \n",
      "11             False            False            False            False   \n",
      "12             False            False            False            False   \n",
      "13             False            False            False            False   \n",
      "14             False            False            False            False   \n",
      "15             False            False            False            False   \n",
      "16             False            False            False            False   \n",
      "17             False            False            False            False   \n",
      "18             False            False            False            False   \n",
      "19             False            False            False            False   \n",
      "20             False            False            False            False   \n",
      "21             False            False            False            False   \n",
      "22             False            False            False            False   \n",
      "23             False            False            False            False   \n",
      "24             False            False            False            False   \n",
      "25             False            False            False            False   \n",
      "26             False            False            False            False   \n",
      "27             False            False            False            False   \n",
      "28             False            False            False            False   \n",
      "29             False            False            False            False   \n",
      "..               ...              ...              ...              ...   \n",
      "917            False            False            False            False   \n",
      "918            False            False            False            False   \n",
      "919            False            False            False            False   \n",
      "920            False            False            False            False   \n",
      "921            False            False            False            False   \n",
      "922            False            False            False            False   \n",
      "923            False            False            False            False   \n",
      "924            False            False            False            False   \n",
      "925            False            False            False            False   \n",
      "926            False            False            False            False   \n",
      "927            False            False            False            False   \n",
      "928            False            False            False            False   \n",
      "929            False            False            False            False   \n",
      "930            False            False            False            False   \n",
      "931            False            False            False            False   \n",
      "932            False            False            False            False   \n",
      "933            False            False            False            False   \n",
      "934            False            False            False            False   \n",
      "935            False            False            False            False   \n",
      "936            False            False            False            False   \n",
      "937            False            False            False            False   \n",
      "938            False            False            False            False   \n",
      "939            False            False            False            False   \n",
      "940            False            False            False            False   \n",
      "941            False            False            False            False   \n",
      "942            False            False            False            False   \n",
      "943            False            False            False            False   \n",
      "944            False            False            False            False   \n",
      "945            False            False            False            False   \n",
      "946            False            False            False            False   \n",
      "\n",
      "     TotalEmojiCharacters  LengthOFFirstParagraph  StylizedLetters  IsGoodNews  \n",
      "0                   False                   False            False       False  \n",
      "1                   False                   False            False       False  \n",
      "2                   False                   False            False       False  \n",
      "3                   False                   False            False       False  \n",
      "4                   False                   False            False       False  \n",
      "5                   False                   False            False       False  \n",
      "6                   False                   False            False       False  \n",
      "7                   False                   False            False       False  \n",
      "8                   False                   False            False       False  \n",
      "9                   False                   False            False       False  \n",
      "10                  False                   False            False       False  \n",
      "11                  False                   False            False       False  \n",
      "12                  False                   False            False       False  \n",
      "13                  False                   False            False       False  \n",
      "14                  False                   False            False       False  \n",
      "15                  False                   False            False       False  \n",
      "16                  False                   False            False       False  \n",
      "17                  False                   False            False       False  \n",
      "18                  False                   False            False       False  \n",
      "19                  False                   False            False       False  \n",
      "20                  False                   False            False       False  \n",
      "21                  False                   False            False       False  \n",
      "22                  False                   False            False       False  \n",
      "23                  False                   False            False       False  \n",
      "24                  False                   False            False       False  \n",
      "25                  False                   False            False       False  \n",
      "26                  False                   False            False       False  \n",
      "27                  False                   False            False       False  \n",
      "28                  False                   False            False       False  \n",
      "29                  False                   False            False       False  \n",
      "..                    ...                     ...              ...         ...  \n",
      "917                 False                   False            False       False  \n",
      "918                 False                   False            False       False  \n",
      "919                 False                   False            False       False  \n",
      "920                 False                   False            False       False  \n",
      "921                 False                   False            False       False  \n",
      "922                 False                   False            False       False  \n",
      "923                 False                   False            False       False  \n",
      "924                 False                   False            False       False  \n",
      "925                 False                   False            False       False  \n",
      "926                 False                   False            False       False  \n",
      "927                 False                   False            False       False  \n",
      "928                 False                   False            False       False  \n",
      "929                 False                   False            False       False  \n",
      "930                 False                   False            False       False  \n",
      "931                 False                   False            False       False  \n",
      "932                 False                   False            False       False  \n",
      "933                 False                   False            False       False  \n",
      "934                 False                   False            False       False  \n",
      "935                 False                   False            False       False  \n",
      "936                 False                   False            False       False  \n",
      "937                 False                   False            False       False  \n",
      "938                 False                   False            False       False  \n",
      "939                 False                   False            False       False  \n",
      "940                 False                   False            False       False  \n",
      "941                 False                   False            False       False  \n",
      "942                 False                   False            False       False  \n",
      "943                 False                   False            False       False  \n",
      "944                 False                   False            False       False  \n",
      "945                 False                   False            False       False  \n",
      "946                 False                   False            False       False  \n",
      "\n",
      "[947 rows x 54 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot compare a dtyped [bool] array with a scalar of type [NoneType]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mror_\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mror_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mor_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'NoneType' and 'bool'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1803\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_binop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m                 except (TypeError, ValueError, AttributeError,\n",
      "\u001b[0;32mpandas/_libs/ops.pyx\u001b[0m in \u001b[0;36mpandas._libs.ops.scalar_binop\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Does not understand character buffer dtype format string ('?')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-36f4fb5765a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mQ1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mIQR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mQ3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mIQR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   2034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2035\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2036\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2038\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_combine_const\u001b[0;34m(self, other, func)\u001b[0m\n\u001b[1;32m   5118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5119\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_to_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mdispatch_to_series\u001b[0;34m(left, right, func, str_rep, axis)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m     \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0muse_numexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mcolumn_op\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcolumn_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m             return {i: func(a.iloc[:, i], b)\n\u001b[0;32m-> 1128\u001b[0;31m                     for i in range(len(a.columns))}\n\u001b[0m\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcolumn_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m             return {i: func(a.iloc[:, i], b)\n\u001b[0;32m-> 1128\u001b[0;31m                     for i in range(len(a.columns))}\n\u001b[0m\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mror_\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mror_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mor_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         filler = (fill_int if is_self_int_dtype and is_other_int_dtype\n\u001b[1;32m   1849\u001b[0m                   else fill_bool)\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0movalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         unfilled = self._constructor(res_values,\n\u001b[1;32m   1852\u001b[0m                                      index=self.index, name=res_name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                                     \u001b[0;34m\"with a scalar of type [{typ}]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                     .format(dtype=x.dtype,\n\u001b[0;32m-> 1810\u001b[0;31m                                             typ=type(y).__name__))\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot compare a dtyped [bool] array with a scalar of type [NoneType]"
     ]
    }
   ],
   "source": [
    "print(data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.207719323320459\n"
     ]
    }
   ],
   "source": [
    "print(data['Freq_Of_Word_2'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freq_Of_Word_1',\n",
       " 'Freq_Of_Word_2',\n",
       " 'Freq_Of_Word_3',\n",
       " 'Freq_Of_Word_4',\n",
       " 'Freq_Of_Word_5',\n",
       " 'Freq_Of_Word_6',\n",
       " 'Freq_Of_Word_7',\n",
       " 'Freq_Of_Word_8',\n",
       " 'Freq_Of_Word_9',\n",
       " 'Freq_Of_Word_10',\n",
       " 'Freq_Of_Word_11',\n",
       " 'Freq_Of_Word_12',\n",
       " 'Freq_Of_Word_13',\n",
       " 'Freq_Of_Word_14',\n",
       " 'Freq_Of_Word_15',\n",
       " 'Freq_Of_Word_16',\n",
       " 'Freq_Of_Word_17',\n",
       " 'Freq_Of_Word_18',\n",
       " 'Freq_Of_Word_19',\n",
       " 'Freq_Of_Word_20',\n",
       " 'Freq_Of_Word_21',\n",
       " 'Freq_Of_Word_22',\n",
       " 'Freq_Of_Word_23',\n",
       " 'Freq_Of_Word_24',\n",
       " 'Freq_Of_Word_25',\n",
       " 'Freq_Of_Word_26',\n",
       " 'Freq_Of_Word_27',\n",
       " 'Freq_Of_Word_28',\n",
       " 'Freq_Of_Word_29',\n",
       " 'Freq_Of_Word_30',\n",
       " 'Freq_Of_Word_31',\n",
       " 'Freq_Of_Word_32',\n",
       " 'Freq_Of_Word_33',\n",
       " 'Freq_Of_Word_34',\n",
       " 'Freq_Of_Word_35',\n",
       " 'Freq_Of_Word_36',\n",
       " 'Freq_Of_Word_37',\n",
       " 'Freq_Of_Word_38',\n",
       " 'Freq_Of_Word_39',\n",
       " 'Freq_Of_Word_40',\n",
       " 'Freq_Of_Word_41',\n",
       " 'Freq_Of_Word_42',\n",
       " 'Freq_Of_Word_43',\n",
       " 'Freq_Of_Word_44',\n",
       " 'Freq_Of_Word_45',\n",
       " 'Freq_Of_Word_46',\n",
       " 'Freq_Of_Word_47',\n",
       " 'Freq_Of_Word_48',\n",
       " 'Freq_Of_Word_49',\n",
       " 'Freq_Of_Word_50',\n",
       " 'TotalEmojiCharacters',\n",
       " 'LengthOFFirstParagraph',\n",
       " 'StylizedLetters']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(data)\n",
    "columns.remove('IsGoodNews')\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columns:\n",
    "    a = data[i].quantile(0.10)\n",
    "    b = data[i].quantile(0.90)\n",
    "    data[i] = np.where(data[i] <a, a,data[i])\n",
    "    data[i] = np.where(data[i] >b, b,data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 54)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    579\n",
      "1    368\n",
      "Name: IsGoodNews, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['IsGoodNews'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.020604774573063\n"
     ]
    }
   ],
   "source": [
    "print(test['Freq_Of_Word_1'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freq_Of_Word_1',\n",
       " 'Freq_Of_Word_2',\n",
       " 'Freq_Of_Word_3',\n",
       " 'Freq_Of_Word_4',\n",
       " 'Freq_Of_Word_5',\n",
       " 'Freq_Of_Word_6',\n",
       " 'Freq_Of_Word_7',\n",
       " 'Freq_Of_Word_8',\n",
       " 'Freq_Of_Word_9',\n",
       " 'Freq_Of_Word_10',\n",
       " 'Freq_Of_Word_11',\n",
       " 'Freq_Of_Word_12',\n",
       " 'Freq_Of_Word_13',\n",
       " 'Freq_Of_Word_14',\n",
       " 'Freq_Of_Word_15',\n",
       " 'Freq_Of_Word_16',\n",
       " 'Freq_Of_Word_17',\n",
       " 'Freq_Of_Word_18',\n",
       " 'Freq_Of_Word_19',\n",
       " 'Freq_Of_Word_20',\n",
       " 'Freq_Of_Word_21',\n",
       " 'Freq_Of_Word_22',\n",
       " 'Freq_Of_Word_23',\n",
       " 'Freq_Of_Word_24',\n",
       " 'Freq_Of_Word_25',\n",
       " 'Freq_Of_Word_26',\n",
       " 'Freq_Of_Word_27',\n",
       " 'Freq_Of_Word_28',\n",
       " 'Freq_Of_Word_29',\n",
       " 'Freq_Of_Word_30',\n",
       " 'Freq_Of_Word_31',\n",
       " 'Freq_Of_Word_32',\n",
       " 'Freq_Of_Word_33',\n",
       " 'Freq_Of_Word_34',\n",
       " 'Freq_Of_Word_35',\n",
       " 'Freq_Of_Word_36',\n",
       " 'Freq_Of_Word_37',\n",
       " 'Freq_Of_Word_38',\n",
       " 'Freq_Of_Word_39',\n",
       " 'Freq_Of_Word_40',\n",
       " 'Freq_Of_Word_41',\n",
       " 'Freq_Of_Word_42',\n",
       " 'Freq_Of_Word_43',\n",
       " 'Freq_Of_Word_44',\n",
       " 'Freq_Of_Word_45',\n",
       " 'Freq_Of_Word_46',\n",
       " 'Freq_Of_Word_47',\n",
       " 'Freq_Of_Word_48',\n",
       " 'Freq_Of_Word_49',\n",
       " 'Freq_Of_Word_50',\n",
       " 'TotalEmojiCharacters',\n",
       " 'LengthOFFirstParagraph',\n",
       " 'StylizedLetters']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnstest = list(test)\n",
    "columnstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columnstest:\n",
    "    a = test[i].quantile(0.10)\n",
    "    b = test[i].quantile(0.90)\n",
    "    test[i] = np.where(test[i] <a, a,test[i])\n",
    "    test[i] = np.where(test[i] >b, b,test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6882474612145235\n"
     ]
    }
   ],
   "source": [
    "print(test['Freq_Of_Word_1'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data.iloc[:,:-1].values\n",
    "y_data = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42) \n",
    "#so that 30% will be selected for testing data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.08001898, -0.31803611, -0.5619523 , ..., -0.08117193,\n",
       "         0.11037028, -0.09208343],\n",
       "       [-0.35186418, -0.31803611, -0.5619523 , ..., -0.10738253,\n",
       "        -0.17587044, -0.11455029],\n",
       "       [-0.35186418, -0.31803611, -0.5619523 , ..., -0.10738253,\n",
       "        -0.19939707, -0.40802362],\n",
       "       ...,\n",
       "       [-0.35186418, -0.31803611, -0.5619523 , ..., -0.10738253,\n",
       "        -0.20331818, -0.40240691],\n",
       "       [-0.35186418, -0.31803611,  1.22306677, ..., -0.05496133,\n",
       "        -0.17587044, -0.31394366],\n",
       "       [-0.35186418, -0.31803611,  0.18339828, ..., -0.08117193,\n",
       "        -0.00726289, -0.2058219 ]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "#resampling need to be done on training dataset only\n",
    "X_train_res, y_train_res = SMOTE().fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       175\n",
      "           1       0.86      0.86      0.86       110\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       285\n",
      "   macro avg       0.89      0.89      0.89       285\n",
      "weighted avg       0.89      0.89      0.89       285\n",
      "\n",
      "0.8947368421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sg = SGDClassifier(random_state=42)\n",
    "sg.fit(X_train_res,y_train_res)\n",
    "pred = sg.predict(X_test)\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "print(classification_report(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#parameter tuning \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#model\n",
    "model = SGDClassifier(random_state=42)\n",
    "#parameters\n",
    "params = {'loss': [\"hinge\", \"log\", \"perceptron\"],\n",
    "          'alpha':[0.001, 0.0001, 0.00001]}\n",
    "#carrying out grid search\n",
    "clf = GridSearchCV(model, params)\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "#the selected parameters by grid search\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#final model by taking suitable parameters\n",
    "clf = SGDClassifier(random_state=42, loss=\"hinge\", alpha=0.001)\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       175\n",
      "           1       0.88      0.84      0.86       110\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       285\n",
      "   macro avg       0.89      0.88      0.88       285\n",
      "weighted avg       0.89      0.89      0.89       285\n",
      "\n",
      "0.8912280701754386\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 527 entries, 0 to 526\n",
      "Data columns (total 1 columns):\n",
      "0    527 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 4.2 KB\n"
     ]
    }
   ],
   "source": [
    "final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527, 53)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_excel('test_output2.xlsx', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-f014ca1dc064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(X_train_res, y_train_res.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.194458\n",
      "         Iterations: 35\n",
      "                             Results: Logit\n",
      "=========================================================================\n",
      "Model:                 Logit              Pseudo R-squared:   0.719      \n",
      "Dependent Variable:    y                  AIC:                386.2437   \n",
      "Date:                  2020-05-04 02:48   BIC:                555.2480   \n",
      "No. Observations:      808                Log-Likelihood:     -157.12    \n",
      "Df Model:              35                 LL-Null:            -560.06    \n",
      "Df Residuals:          772                LLR p-value:        1.1936e-146\n",
      "Converged:             0.0000             Scale:              1.0000     \n",
      "No. Iterations:        35.0000                                           \n",
      "-------------------------------------------------------------------------\n",
      "     Coef.      Std.Err.       z    P>|z|       [0.025         0.975]    \n",
      "-------------------------------------------------------------------------\n",
      "x1   -0.5924         0.4475 -1.3238 0.1856         -1.4694         0.2846\n",
      "x2    0.2534         0.4984  0.5085 0.6111         -0.7233         1.2302\n",
      "x3    0.2089         0.2643  0.7904 0.4293         -0.3091         0.7270\n",
      "x4    0.4376 192979099.9291  0.0000 1.0000 -378232085.1923 378232086.0675\n",
      "x5    0.7593         0.3356  2.2626 0.0237          0.1016         1.4170\n",
      "x6   -0.0384         0.4488 -0.0855 0.9318         -0.9179         0.8412\n",
      "x7    3.0283         0.6965  4.3479 0.0000          1.6632         4.3934\n",
      "x8    3.7791         1.2050  3.1363 0.0017          1.4174         6.1408\n",
      "x9    0.4328         0.5204  0.8316 0.4056         -0.5872         1.4528\n",
      "x10  -0.2470         0.5552 -0.4448 0.6565         -1.3352         0.8413\n",
      "x11  -0.4362         0.5503 -0.7925 0.4281         -1.5148         0.6425\n",
      "x12  -0.7280         0.2938 -2.4783 0.0132         -1.3038        -0.1523\n",
      "x13  -0.3140         0.5147 -0.6101 0.5418         -1.3227         0.6947\n",
      "x14   2.0570  21851681.1342  0.0000 1.0000  -42828505.9676  42828510.0817\n",
      "x15   2.1183  31184782.1866  0.0000 1.0000  -61121047.8333  61121052.0698\n",
      "x16   1.5476         0.5475  2.8265 0.0047          0.4745         2.6207\n",
      "x17   0.5667         0.6621  0.8560 0.3920         -0.7309         1.8643\n",
      "x18  -0.1352         0.4458 -0.3032 0.7617         -1.0089         0.7385\n",
      "x19  -0.0205         0.2243 -0.0916 0.9270         -0.4601         0.4190\n",
      "x20  26.2987        13.7549  1.9120 0.0559         -0.6603        53.2578\n",
      "x21   0.9325         0.2855  3.2658 0.0011          0.3729         1.4921\n",
      "x22   1.4832            nan     nan    nan             nan            nan\n",
      "x23   1.6592         1.0488  1.5820 0.1136         -0.3964         3.7148\n",
      "x24   4.1345         1.3701  3.0176 0.0025          1.4491         6.8198\n",
      "x25  -6.7287         1.4491 -4.6434 0.0000         -9.5688        -3.8885\n",
      "x26   0.6242         0.9405  0.6637 0.5069         -1.2191         2.4675\n",
      "x27 -44.4697        12.7640 -3.4840 0.0005        -69.4868       -19.4526\n",
      "x28   2.5541  23973378.8065  0.0000 1.0000  -46986956.4944  46986961.6026\n",
      "x29   1.8407  20871121.8391  0.0000 1.0000  -40906645.2808  40906648.9623\n",
      "x30   3.5761         4.5421  0.7873 0.4311         -5.3262        12.4785\n",
      "x31   1.6852  22183105.8323  0.0000 1.0000  -43478086.8114  43478090.1818\n",
      "x32   1.6830  56976537.7901  0.0000 1.0000 -111671960.3494 111671963.7154\n",
      "x33   2.0422  20061644.1975  0.0000 1.0000  -39320098.0556  39320102.1400\n",
      "x34   1.6999  42703688.8006  0.0000 1.0000  -83697690.3563  83697693.7561\n",
      "x35 -13.3606         7.0627 -1.8917 0.0585        -27.2032         0.4820\n",
      "x36   2.5293         1.1510  2.1976 0.0280          0.2735         4.7851\n",
      "x37  -2.4018         0.6717 -3.5756 0.0003         -3.7183        -1.0852\n",
      "x38   0.6194            nan     nan    nan             nan            nan\n",
      "x39   2.0238  26385348.9614  0.0000 1.0000  -51714331.6601  51714335.7077\n",
      "x40  10.1973         8.4079  1.2128 0.2252         -6.2819        26.6766\n",
      "x41   1.4936  14775181.4212  0.0000 1.0000  -28958821.9570  28958824.9442\n",
      "x42   1.8854            nan     nan    nan             nan            nan\n",
      "x43   2.3279  18993425.9727  0.0000 1.0000  -37226428.5215  37226433.1774\n",
      "x44   1.3433  28972312.6761  0.0000 1.0000  -56784688.0506  56784690.7372\n",
      "x45  -1.4110         0.5159 -2.7352 0.0062         -2.4221        -0.3999\n",
      "x46  -9.7339         3.5700 -2.7266 0.0064        -16.7309        -2.7368\n",
      "x47   0.8873  28133429.9153  0.0000 1.0000  -55140508.5084  55140510.2829\n",
      "x48   1.3242            nan     nan    nan             nan            nan\n",
      "x49  -3.5048         2.1242 -1.6500 0.0990         -7.6682         0.6585\n",
      "x50   0.3521         0.4030  0.8736 0.3823         -0.4378         1.1420\n",
      "x51  19.9506         5.3642  3.7192 0.0002          9.4369        30.4642\n",
      "x52  -2.4188         2.7354 -0.8843 0.3766         -7.7800         2.9424\n",
      "x53   2.5641         1.1481  2.2334 0.0255          0.3139         4.8144\n",
      "=========================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py:1092: RuntimeWarning: invalid value encountered in sqrt\n",
      "  bse_ = np.sqrt(np.diag(self.cov_params()))\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y_train_res,X_train_res)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-197-75105bfec389>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-197-75105bfec389>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Cols = [â€˜Freq_Of_Word_5â€™, â€˜Freq_Of_Word_7â€™, â€˜Freq_Of_Word_8â€™, â€˜Freq_Of_Word_12â€™, â€˜Freq_Of_Word_16â€™, â€˜Freq_Of_Word_21â€™, â€˜Freq_Of_Word_24â€™, â€˜Freq_Of_Word_25â€™, â€˜Freq_Of_Word_27â€™, â€˜Freq_Of_Word_36â€™, â€˜Freq_Of_Word_37â€™, â€˜Freq_Of_Word_45â€™, â€˜Freq_Of_Word_46â€™, 'TotalEmojiCharacters', â€˜StylizedLettersâ€™]\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "Cols = [â€˜Freq_Of_Word_5â€™, â€˜Freq_Of_Word_7â€™, â€˜Freq_Of_Word_8â€™, â€˜Freq_Of_Word_12â€™, â€˜Freq_Of_Word_16â€™, â€˜Freq_Of_Word_21â€™, â€˜Freq_Of_Word_24â€™, â€˜Freq_Of_Word_25â€™, â€˜Freq_Of_Word_27â€™, â€˜Freq_Of_Word_36â€™, â€˜Freq_Of_Word_37â€™, â€˜Freq_Of_Word_45â€™, â€˜Freq_Of_Word_46â€™, 'TotalEmojiCharacters', â€˜StylizedLettersâ€™]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-bbb86d038309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "X = X_train_res.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urvashigupta/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "X_train_res, X_test, y_train_res, y_test = train_test_split(X_train_res, y_train_res, test_size=0.3, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.89\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[109  11]\n",
      " [ 16 107]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       120\n",
      "           1       0.91      0.87      0.89       123\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       243\n",
      "   macro avg       0.89      0.89      0.89       243\n",
      "weighted avg       0.89      0.89      0.89       243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_finallog = logreg.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2 = pd.DataFrame(pred_finallog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2.to_excel('test_output3.xlsx', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
